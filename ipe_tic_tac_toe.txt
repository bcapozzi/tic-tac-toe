GENERATING EPISODE 1----------
PRIOR TO MOVE:
['-', '-', '-']
['-', '-', '-']
['-', '-', '-']
policy by action: {0: 0.1111111111111111, 1: 0.1111111111111111, 2: 0.1111111111111111, 3: 0.1111111111111111, 4: 0.1111111111111111, 5: 0.1111111111111111, 6: 0.1111111111111111, 7: 0.1111111111111111, 8: 0.1111111111111111}
Action: [0, 1, 2, 3, 4, 5, 6, 7, 8]
PROBABILITIES: [0.1111111111111111, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.5555555555555556, 0.6666666666666667, 0.7777777777777779, 0.8888888888888891, 1.0000000000000002]
Selected action: 4
AGENT MAKING MOVE: (1, 1)4
AFTER AGENT MOVE:
['-', '-', '-']
['-', 'X', '-']
['-', '-', '-']
AFTER OPPONENT MOVE
['-', '-', '-']
['-', 'X', '-']
['O', '-', '-']
ADDING TRANSITION: {'from_state': [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']], 'to_state': [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']], 'action': (1, 1), 'reward': 0.0}
PRIOR TO MOVE:
['-', '-', '-']
['-', 'X', '-']
['O', '-', '-']
policy by action: {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 5: 0.14285714285714285, 7: 0.14285714285714285, 8: 0.14285714285714285}
Action: [0, 1, 2, 3, 5, 7, 8]
PROBABILITIES: [0.14285714285714285, 0.2857142857142857, 0.42857142857142855, 0.5714285714285714, 0.7142857142857142, 0.857142857142857, 0.9999999999999998]
Selected action: 7
AGENT MAKING MOVE: (2, 1)7
AFTER AGENT MOVE:
['-', '-', '-']
['-', 'X', '-']
['O', 'X', '-']
AFTER OPPONENT MOVE
['-', '-', '-']
['O', 'X', '-']
['O', 'X', '-']
ADDING TRANSITION: {'from_state': [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']], 'to_state': [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']], 'action': (2, 1), 'reward': 0.0}
PRIOR TO MOVE:
['-', '-', '-']
['O', 'X', '-']
['O', 'X', '-']
policy by action: {0: 0.2, 1: 0.2, 2: 0.2, 5: 0.2, 8: 0.2}
Action: [0, 1, 2, 5, 8]
PROBABILITIES: [0.2, 0.4, 0.6000000000000001, 0.8, 1.0]
Selected action: 2
AGENT MAKING MOVE: (0, 2)2
AFTER AGENT MOVE:
['-', '-', 'X']
['O', 'X', '-']
['O', 'X', '-']
AFTER OPPONENT MOVE
['O', '-', 'X']
['O', 'X', '-']
['O', 'X', '-']
ADDING TRANSITION: {'from_state': [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']], 'to_state': [['O', '-', 'X'], ['O', 'X', '-'], ['O', 'X', '-']], 'action': (0, 2), 'reward': -1.0}
BOARD AT END OF EPISODE
['O', '-', 'X']
['O', 'X', '-']
['O', 'X', '-']
Computing future returns for transition [0] : {'from_state': [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']], 'to_state': [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']], 'action': (1, 1), 'reward': 0.0}
Reward for transition[0] : 0.0 , discount: 1.0
Reward for transition[1] : 0.0 , discount: 0.95
Reward for transition[2] : -1.0 , discount: 0.9025
Reward history for state [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']] / action 4 --> 1 : [-0.9025]
state [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']] not yet in Q(s,a) --> initializing empty map
Updating value for state [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']] / action: 8 to: -0.9025
Computing future returns for transition [1] : {'from_state': [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']], 'to_state': [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']], 'action': (2, 1), 'reward': 0.0}
Reward for transition[1] : 0.0 , discount: 1.0
Reward for transition[2] : -1.0 , discount: 0.95
Reward history for state [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']] / action 7 --> 1 : [-0.95]
state [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']] not yet in Q(s,a) --> initializing empty map
Updating value for state [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']] / action: 8 to: -0.95
Computing future returns for transition [2] : {'from_state': [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']], 'to_state': [['O', '-', 'X'], ['O', 'X', '-'], ['O', 'X', '-']], 'action': (0, 2), 'reward': -1.0}
Reward for transition[2] : -1.0 , discount: 1.0
Reward history for state [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']] / action 2 --> 1 : [-1.0]
state [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']] not yet in Q(s,a) --> initializing empty map
Updating value for state [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']] / action: 8 to: -1.0
PREVIOUS POLICY: 
[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]
p(a): {0: 0.1111111111111111, 1: 0.1111111111111111, 2: 0.1111111111111111, 3: 0.1111111111111111, 4: 0.1111111111111111, 5: 0.1111111111111111, 6: 0.1111111111111111, 7: 0.1111111111111111, 8: 0.1111111111111111}
[['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']]
p(a): {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 5: 0.14285714285714285, 7: 0.14285714285714285, 8: 0.14285714285714285}
[['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']]
p(a): {0: 0.2, 1: 0.2, 2: 0.2, 5: 0.2, 8: 0.2}
['-', '-', '-']
['-', '-', '-']
['-', '-', '-']
ValuesByAction for state: [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']] --> {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: -0.9025}
values --> [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9025]
sorted indices: [8 0 1 2 3 4 5 6 7]
UPDATE POLICY:  ASTAR ==> 7
POSSIBLE ACTIONS: {0, 1, 2, 3, 4, 5, 6, 7, 8}
UPDATED POLICY: {"[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]": {0: 0.00011111111111111112, 1: 0.00011111111111111112, 2: 0.00011111111111111112, 3: 0.00011111111111111112, 4: 0.00011111111111111112, 5: 0.00011111111111111112, 6: 0.00011111111111111112, 7: 0.9991111111111111, 8: 0.00011111111111111112}, "[['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']]": {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 5: 0.14285714285714285, 7: 0.14285714285714285, 8: 0.14285714285714285}, "[['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']]": {0: 0.2, 1: 0.2, 2: 0.2, 5: 0.2, 8: 0.2}}
['-', '-', '-']
['-', 'X', '-']
['O', '-', '-']
ValuesByAction for state: [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']] --> {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 7: 0.0, 8: -0.95}
values --> [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.95]
sorted indices: [6 0 1 2 3 4 5]
UPDATE POLICY:  ASTAR ==> 7
POSSIBLE ACTIONS: {0, 1, 2, 3, 5, 7, 8}
UPDATED POLICY: {"[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]": {0: 0.00011111111111111112, 1: 0.00011111111111111112, 2: 0.00011111111111111112, 3: 0.00011111111111111112, 4: 0.00011111111111111112, 5: 0.00011111111111111112, 6: 0.00011111111111111112, 7: 0.9991111111111111, 8: 0.00011111111111111112}, "[['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']]": {0: 0.00014285714285714287, 1: 0.00014285714285714287, 2: 0.00014285714285714287, 3: 0.00014285714285714287, 5: 0.00014285714285714287, 7: 0.9991428571428571, 8: 0.00014285714285714287}, "[['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']]": {0: 0.2, 1: 0.2, 2: 0.2, 5: 0.2, 8: 0.2}}
['-', '-', '-']
['O', 'X', '-']
['O', 'X', '-']
ValuesByAction for state: [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']] --> {0: 0.0, 1: 0.0, 2: 0.0, 5: 0.0, 8: -1.0}
values --> [0.0, 0.0, 0.0, 0.0, -1.0]
sorted indices: [4 0 1 2 3]
UPDATE POLICY:  ASTAR ==> 5
POSSIBLE ACTIONS: {0, 1, 2, 5, 8}
UPDATED POLICY: {"[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]": {0: 0.00011111111111111112, 1: 0.00011111111111111112, 2: 0.00011111111111111112, 3: 0.00011111111111111112, 4: 0.00011111111111111112, 5: 0.00011111111111111112, 6: 0.00011111111111111112, 7: 0.9991111111111111, 8: 0.00011111111111111112}, "[['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']]": {0: 0.00014285714285714287, 1: 0.00014285714285714287, 2: 0.00014285714285714287, 3: 0.00014285714285714287, 5: 0.00014285714285714287, 7: 0.9991428571428571, 8: 0.00014285714285714287}, "[['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']]": {0: 0.0002, 1: 0.0002, 2: 0.0002, 5: 0.9992, 8: 0.0002}}
