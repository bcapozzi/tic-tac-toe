GENERATING EPISODE 1----------
PRIOR TO MOVE:
['-', '-', '-']
['-', '-', '-']
['-', '-', '-']
policy by action: {0: 0.1111111111111111, 1: 0.1111111111111111, 2: 0.1111111111111111, 3: 0.1111111111111111, 4: 0.1111111111111111, 5: 0.1111111111111111, 6: 0.1111111111111111, 7: 0.1111111111111111, 8: 0.1111111111111111}
Action: [0, 1, 2, 3, 4, 5, 6, 7, 8]
PROBABILITIES: [0.1111111111111111, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.5555555555555556, 0.6666666666666667, 0.7777777777777779, 0.8888888888888891, 1.0000000000000002]
Selected action: 4
AGENT MAKING MOVE: (1, 1)4
AFTER AGENT MOVE:
['-', '-', '-']
['-', 'X', '-']
['-', '-', '-']
AFTER OPPONENT MOVE
['-', '-', '-']
['-', 'X', '-']
['O', '-', '-']
ADDING TRANSITION: {'from_state': [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']], 'to_state': [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']], 'action': (1, 1), 'reward': 0.0}
PRIOR TO MOVE:
['-', '-', '-']
['-', 'X', '-']
['O', '-', '-']
policy by action: {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 5: 0.14285714285714285, 7: 0.14285714285714285, 8: 0.14285714285714285}
Action: [0, 1, 2, 3, 5, 7, 8]
PROBABILITIES: [0.14285714285714285, 0.2857142857142857, 0.42857142857142855, 0.5714285714285714, 0.7142857142857142, 0.857142857142857, 0.9999999999999998]
Selected action: 7
AGENT MAKING MOVE: (2, 1)7
AFTER AGENT MOVE:
['-', '-', '-']
['-', 'X', '-']
['O', 'X', '-']
AFTER OPPONENT MOVE
['-', '-', '-']
['O', 'X', '-']
['O', 'X', '-']
ADDING TRANSITION: {'from_state': [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']], 'to_state': [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']], 'action': (2, 1), 'reward': 0.0}
PRIOR TO MOVE:
['-', '-', '-']
['O', 'X', '-']
['O', 'X', '-']
policy by action: {0: 0.2, 1: 0.2, 2: 0.2, 5: 0.2, 8: 0.2}
Action: [0, 1, 2, 5, 8]
PROBABILITIES: [0.2, 0.4, 0.6000000000000001, 0.8, 1.0]
Selected action: 2
AGENT MAKING MOVE: (0, 2)2
AFTER AGENT MOVE:
['-', '-', 'X']
['O', 'X', '-']
['O', 'X', '-']
AFTER OPPONENT MOVE
['O', '-', 'X']
['O', 'X', '-']
['O', 'X', '-']
ADDING TRANSITION: {'from_state': [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']], 'to_state': [['O', '-', 'X'], ['O', 'X', '-'], ['O', 'X', '-']], 'action': (0, 2), 'reward': -1.0}
BOARD AT END OF EPISODE
['O', '-', 'X']
['O', 'X', '-']
['O', 'X', '-']
Computing future returns for transition [0] : {'from_state': [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']], 'to_state': [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']], 'action': (1, 1), 'reward': 0.0}
Reward for transition[0] : 0.0 , discount: 1.0
Reward for transition[1] : 0.0 , discount: 0.95
Reward for transition[2] : -1.0 , discount: 0.9025
Reward history for state [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']] / action 4 --> 1 : [-0.9025]
state [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']] not yet in Q(s,a) --> initializing empty map
Updating value for state [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']] / action: 8 to: -0.9025
Computing future returns for transition [1] : {'from_state': [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']], 'to_state': [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']], 'action': (2, 1), 'reward': 0.0}
Reward for transition[1] : 0.0 , discount: 1.0
Reward for transition[2] : -1.0 , discount: 0.95
Reward history for state [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']] / action 7 --> 1 : [-0.95]
state [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']] not yet in Q(s,a) --> initializing empty map
Updating value for state [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']] / action: 8 to: -0.95
Computing future returns for transition [2] : {'from_state': [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']], 'to_state': [['O', '-', 'X'], ['O', 'X', '-'], ['O', 'X', '-']], 'action': (0, 2), 'reward': -1.0}
Reward for transition[2] : -1.0 , discount: 1.0
Reward history for state [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']] / action 2 --> 1 : [-1.0]
state [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']] not yet in Q(s,a) --> initializing empty map
Updating value for state [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']] / action: 8 to: -1.0
PREVIOUS POLICY: 
[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]
p(a): {0: 0.1111111111111111, 1: 0.1111111111111111, 2: 0.1111111111111111, 3: 0.1111111111111111, 4: 0.1111111111111111, 5: 0.1111111111111111, 6: 0.1111111111111111, 7: 0.1111111111111111, 8: 0.1111111111111111}
[['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']]
p(a): {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 5: 0.14285714285714285, 7: 0.14285714285714285, 8: 0.14285714285714285}
[['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']]
p(a): {0: 0.2, 1: 0.2, 2: 0.2, 5: 0.2, 8: 0.2}
['-', '-', '-']
['-', '-', '-']
['-', '-', '-']
ValuesByAction for state: [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']] --> {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: -0.9025}
values --> [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9025]
sorted indices: [8 0 1 2 3 4 5 6 7]
UPDATE POLICY:  ASTAR ==> 7
POSSIBLE ACTIONS: {0, 1, 2, 3, 4, 5, 6, 7, 8}
UPDATED POLICY: {"[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]": {0: 0.00011111111111111112, 1: 0.00011111111111111112, 2: 0.00011111111111111112, 3: 0.00011111111111111112, 4: 0.00011111111111111112, 5: 0.00011111111111111112, 6: 0.00011111111111111112, 7: 0.9991111111111111, 8: 0.00011111111111111112}, "[['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']]": {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 5: 0.14285714285714285, 7: 0.14285714285714285, 8: 0.14285714285714285}, "[['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']]": {0: 0.2, 1: 0.2, 2: 0.2, 5: 0.2, 8: 0.2}}
['-', '-', '-']
['-', 'X', '-']
['O', '-', '-']
ValuesByAction for state: [['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']] --> {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.0, 7: 0.0, 8: -0.95}
values --> [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.95]
sorted indices: [6 0 1 2 3 4 5]
UPDATE POLICY:  ASTAR ==> 7
POSSIBLE ACTIONS: {0, 1, 2, 3, 5, 7, 8}
UPDATED POLICY: {"[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]": {0: 0.00011111111111111112, 1: 0.00011111111111111112, 2: 0.00011111111111111112, 3: 0.00011111111111111112, 4: 0.00011111111111111112, 5: 0.00011111111111111112, 6: 0.00011111111111111112, 7: 0.9991111111111111, 8: 0.00011111111111111112}, "[['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']]": {0: 0.00014285714285714287, 1: 0.00014285714285714287, 2: 0.00014285714285714287, 3: 0.00014285714285714287, 5: 0.00014285714285714287, 7: 0.9991428571428571, 8: 0.00014285714285714287}, "[['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']]": {0: 0.2, 1: 0.2, 2: 0.2, 5: 0.2, 8: 0.2}}
['-', '-', '-']
['O', 'X', '-']
['O', 'X', '-']
ValuesByAction for state: [['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']] --> {0: 0.0, 1: 0.0, 2: 0.0, 5: 0.0, 8: -1.0}
values --> [0.0, 0.0, 0.0, 0.0, -1.0]
sorted indices: [4 0 1 2 3]
UPDATE POLICY:  ASTAR ==> 5
POSSIBLE ACTIONS: {0, 1, 2, 5, 8}
UPDATED POLICY: {"[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]": {0: 0.00011111111111111112, 1: 0.00011111111111111112, 2: 0.00011111111111111112, 3: 0.00011111111111111112, 4: 0.00011111111111111112, 5: 0.00011111111111111112, 6: 0.00011111111111111112, 7: 0.9991111111111111, 8: 0.00011111111111111112}, "[['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']]": {0: 0.00014285714285714287, 1: 0.00014285714285714287, 2: 0.00014285714285714287, 3: 0.00014285714285714287, 5: 0.00014285714285714287, 7: 0.9991428571428571, 8: 0.00014285714285714287}, "[['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']]": {0: 0.0002, 1: 0.0002, 2: 0.0002, 5: 0.9992, 8: 0.0002}}
GENERATING EPISODE 2----------
PRIOR TO MOVE:
['-', '-', '-']
['-', '-', '-']
['-', '-', '-']
policy by action: {0: 0.00011111111111111112, 1: 0.00011111111111111112, 2: 0.00011111111111111112, 3: 0.00011111111111111112, 4: 0.00011111111111111112, 5: 0.00011111111111111112, 6: 0.00011111111111111112, 7: 0.9991111111111111, 8: 0.00011111111111111112}
Action: [0, 1, 2, 3, 4, 5, 6, 7, 8]
PROBABILITIES: [0.00011111111111111112, 0.00022222222222222223, 0.0003333333333333334, 0.00044444444444444447, 0.0005555555555555556, 0.0006666666666666666, 0.0007777777777777777, 0.9998888888888888, 0.9999999999999999]
Selected action: 7
AGENT MAKING MOVE: (2, 1)7
AFTER AGENT MOVE:
['-', '-', '-']
['-', '-', '-']
['-', 'X', '-']
AFTER OPPONENT MOVE
['-', '-', '-']
['-', '-', 'O']
['-', 'X', '-']
ADDING TRANSITION: {'from_state': [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']], 'to_state': [['-', '-', '-'], ['-', '-', 'O'], ['-', 'X', '-']], 'action': (2, 1), 'reward': 0.0}
PRIOR TO MOVE:
['-', '-', '-']
['-', '-', 'O']
['-', 'X', '-']
policy by action: {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 4: 0.14285714285714285, 6: 0.14285714285714285, 8: 0.14285714285714285}
Action: [0, 1, 2, 3, 4, 6, 8]
PROBABILITIES: [0.14285714285714285, 0.2857142857142857, 0.42857142857142855, 0.5714285714285714, 0.7142857142857142, 0.857142857142857, 0.9999999999999998]
Selected action: 2
AGENT MAKING MOVE: (0, 2)2
AFTER AGENT MOVE:
['-', '-', 'X']
['-', '-', 'O']
['-', 'X', '-']
AFTER OPPONENT MOVE
['O', '-', 'X']
['-', '-', 'O']
['-', 'X', '-']
ADDING TRANSITION: {'from_state': [['-', '-', '-'], ['-', '-', 'O'], ['-', 'X', '-']], 'to_state': [['O', '-', 'X'], ['-', '-', 'O'], ['-', 'X', '-']], 'action': (0, 2), 'reward': 0.0}
PRIOR TO MOVE:
['O', '-', 'X']
['-', '-', 'O']
['-', 'X', '-']
policy by action: {1: 0.2, 3: 0.2, 4: 0.2, 6: 0.2, 8: 0.2}
Action: [1, 3, 4, 6, 8]
PROBABILITIES: [0.2, 0.4, 0.6000000000000001, 0.8, 1.0]
Selected action: 8
AGENT MAKING MOVE: (2, 2)8
AFTER AGENT MOVE:
['O', '-', 'X']
['-', '-', 'O']
['-', 'X', 'X']
AFTER OPPONENT MOVE
['O', '-', 'X']
['-', 'O', 'O']
['-', 'X', 'X']
ADDING TRANSITION: {'from_state': [['O', '-', 'X'], ['-', '-', 'O'], ['-', 'X', '-']], 'to_state': [['O', '-', 'X'], ['-', 'O', 'O'], ['-', 'X', 'X']], 'action': (2, 2), 'reward': 0.0}
PRIOR TO MOVE:
['O', '-', 'X']
['-', 'O', 'O']
['-', 'X', 'X']
policy by action: {1: 0.3333333333333333, 3: 0.3333333333333333, 6: 0.3333333333333333}
Action: [1, 3, 6]
PROBABILITIES: [0.3333333333333333, 0.6666666666666666, 1.0]
Selected action: 3
AGENT MAKING MOVE: (1, 0)3
AFTER AGENT MOVE:
['O', '-', 'X']
['X', 'O', 'O']
['-', 'X', 'X']
AFTER OPPONENT MOVE
['O', '-', 'X']
['X', 'O', 'O']
['O', 'X', 'X']
ADDING TRANSITION: {'from_state': [['O', '-', 'X'], ['-', 'O', 'O'], ['-', 'X', 'X']], 'to_state': [['O', '-', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']], 'action': (1, 0), 'reward': 0.0}
PRIOR TO MOVE:
['O', '-', 'X']
['X', 'O', 'O']
['O', 'X', 'X']
policy by action: {1: 1.0}
Action: [1]
PROBABILITIES: [1.0]
Selected action: 1
AGENT MAKING MOVE: (0, 1)1
AFTER AGENT MOVE:
['O', 'X', 'X']
['X', 'O', 'O']
['O', 'X', 'X']
ADDING TRANSITION: {'from_state': [['O', '-', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']], 'to_state': [['O', 'X', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']], 'action': (0, 1), 'reward': 0.0}
BOARD AT END OF EPISODE
['O', 'X', 'X']
['X', 'O', 'O']
['O', 'X', 'X']
Computing future returns for transition [0] : {'from_state': [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']], 'to_state': [['-', '-', '-'], ['-', '-', 'O'], ['-', 'X', '-']], 'action': (2, 1), 'reward': 0.0}
Reward for transition[0] : 0.0 , discount: 1.0
Reward for transition[1] : 0.0 , discount: 0.95
Reward for transition[2] : 0.0 , discount: 0.9025
Reward for transition[3] : 0.0 , discount: 0.8573749999999999
Reward for transition[4] : 0.0 , discount: 0.8145062499999999
Reward history for state [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']] / action 7 --> 1 : [0.0]
Updating value for state [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']] / action: 7 to: 0.0
Computing future returns for transition [1] : {'from_state': [['-', '-', '-'], ['-', '-', 'O'], ['-', 'X', '-']], 'to_state': [['O', '-', 'X'], ['-', '-', 'O'], ['-', 'X', '-']], 'action': (0, 2), 'reward': 0.0}
Reward for transition[1] : 0.0 , discount: 1.0
Reward for transition[2] : 0.0 , discount: 0.95
Reward for transition[3] : 0.0 , discount: 0.9025
Reward for transition[4] : 0.0 , discount: 0.8573749999999999
Reward history for state [['-', '-', '-'], ['-', '-', 'O'], ['-', 'X', '-']] / action 2 --> 1 : [0.0]
state [['-', '-', '-'], ['-', '-', 'O'], ['-', 'X', '-']] not yet in Q(s,a) --> initializing empty map
Updating value for state [['-', '-', '-'], ['-', '-', 'O'], ['-', 'X', '-']] / action: 8 to: 0.0
Computing future returns for transition [2] : {'from_state': [['O', '-', 'X'], ['-', '-', 'O'], ['-', 'X', '-']], 'to_state': [['O', '-', 'X'], ['-', 'O', 'O'], ['-', 'X', 'X']], 'action': (2, 2), 'reward': 0.0}
Reward for transition[2] : 0.0 , discount: 1.0
Reward for transition[3] : 0.0 , discount: 0.95
Reward for transition[4] : 0.0 , discount: 0.9025
Reward history for state [['O', '-', 'X'], ['-', '-', 'O'], ['-', 'X', '-']] / action 8 --> 1 : [0.0]
state [['O', '-', 'X'], ['-', '-', 'O'], ['-', 'X', '-']] not yet in Q(s,a) --> initializing empty map
Updating value for state [['O', '-', 'X'], ['-', '-', 'O'], ['-', 'X', '-']] / action: 8 to: 0.0
Computing future returns for transition [3] : {'from_state': [['O', '-', 'X'], ['-', 'O', 'O'], ['-', 'X', 'X']], 'to_state': [['O', '-', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']], 'action': (1, 0), 'reward': 0.0}
Reward for transition[3] : 0.0 , discount: 1.0
Reward for transition[4] : 0.0 , discount: 0.95
Reward history for state [['O', '-', 'X'], ['-', 'O', 'O'], ['-', 'X', 'X']] / action 3 --> 1 : [0.0]
state [['O', '-', 'X'], ['-', 'O', 'O'], ['-', 'X', 'X']] not yet in Q(s,a) --> initializing empty map
Updating value for state [['O', '-', 'X'], ['-', 'O', 'O'], ['-', 'X', 'X']] / action: 6 to: 0.0
Computing future returns for transition [4] : {'from_state': [['O', '-', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']], 'to_state': [['O', 'X', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']], 'action': (0, 1), 'reward': 0.0}
Reward for transition[4] : 0.0 , discount: 1.0
Reward history for state [['O', '-', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']] / action 1 --> 1 : [0.0]
state [['O', '-', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']] not yet in Q(s,a) --> initializing empty map
Updating value for state [['O', '-', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']] / action: 1 to: 0.0
PREVIOUS POLICY: 
[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]
p(a): {0: 0.00011111111111111112, 1: 0.00011111111111111112, 2: 0.00011111111111111112, 3: 0.00011111111111111112, 4: 0.00011111111111111112, 5: 0.00011111111111111112, 6: 0.00011111111111111112, 7: 0.9991111111111111, 8: 0.00011111111111111112}
[['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']]
p(a): {0: 0.00014285714285714287, 1: 0.00014285714285714287, 2: 0.00014285714285714287, 3: 0.00014285714285714287, 5: 0.00014285714285714287, 7: 0.9991428571428571, 8: 0.00014285714285714287}
[['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']]
p(a): {0: 0.0002, 1: 0.0002, 2: 0.0002, 5: 0.9992, 8: 0.0002}
[['-', '-', '-'], ['-', '-', 'O'], ['-', 'X', '-']]
p(a): {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 4: 0.14285714285714285, 6: 0.14285714285714285, 8: 0.14285714285714285}
[['O', '-', 'X'], ['-', '-', 'O'], ['-', 'X', '-']]
p(a): {1: 0.2, 3: 0.2, 4: 0.2, 6: 0.2, 8: 0.2}
[['O', '-', 'X'], ['-', 'O', 'O'], ['-', 'X', 'X']]
p(a): {1: 0.3333333333333333, 3: 0.3333333333333333, 6: 0.3333333333333333}
[['O', '-', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']]
p(a): {1: 1.0}
['-', '-', '-']
['-', '-', '-']
['-', '-', '-']
ValuesByAction for state: [['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']] --> {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: -0.9025}
values --> [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9025]
sorted indices: [8 0 1 2 3 4 5 6 7]
UPDATE POLICY:  ASTAR ==> 7
POSSIBLE ACTIONS: {0, 1, 2, 3, 4, 5, 6, 7, 8}
UPDATED POLICY: {"[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]": {0: 0.00011111111111111112, 1: 0.00011111111111111112, 2: 0.00011111111111111112, 3: 0.00011111111111111112, 4: 0.00011111111111111112, 5: 0.00011111111111111112, 6: 0.00011111111111111112, 7: 0.9991111111111111, 8: 0.00011111111111111112}, "[['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']]": {0: 0.00014285714285714287, 1: 0.00014285714285714287, 2: 0.00014285714285714287, 3: 0.00014285714285714287, 5: 0.00014285714285714287, 7: 0.9991428571428571, 8: 0.00014285714285714287}, "[['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']]": {0: 0.0002, 1: 0.0002, 2: 0.0002, 5: 0.9992, 8: 0.0002}, "[['-', '-', '-'], ['-', '-', 'O'], ['-', 'X', '-']]": {0: 0.14285714285714285, 1: 0.14285714285714285, 2: 0.14285714285714285, 3: 0.14285714285714285, 4: 0.14285714285714285, 6: 0.14285714285714285, 8: 0.14285714285714285}, "[['O', '-', 'X'], ['-', '-', 'O'], ['-', 'X', '-']]": {1: 0.2, 3: 0.2, 4: 0.2, 6: 0.2, 8: 0.2}, "[['O', '-', 'X'], ['-', 'O', 'O'], ['-', 'X', 'X']]": {1: 0.3333333333333333, 3: 0.3333333333333333, 6: 0.3333333333333333}, "[['O', '-', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']]": {1: 1.0}}
['-', '-', '-']
['-', '-', 'O']
['-', 'X', '-']
ValuesByAction for state: [['-', '-', '-'], ['-', '-', 'O'], ['-', 'X', '-']] --> {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 6: 0.0, 8: 0.0}
values --> [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
sorted indices: [0 1 2 3 4 5 6]
UPDATE POLICY:  ASTAR ==> 8
POSSIBLE ACTIONS: {0, 1, 2, 3, 4, 6, 8}
UPDATED POLICY: {"[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]": {0: 0.00011111111111111112, 1: 0.00011111111111111112, 2: 0.00011111111111111112, 3: 0.00011111111111111112, 4: 0.00011111111111111112, 5: 0.00011111111111111112, 6: 0.00011111111111111112, 7: 0.9991111111111111, 8: 0.00011111111111111112}, "[['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']]": {0: 0.00014285714285714287, 1: 0.00014285714285714287, 2: 0.00014285714285714287, 3: 0.00014285714285714287, 5: 0.00014285714285714287, 7: 0.9991428571428571, 8: 0.00014285714285714287}, "[['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']]": {0: 0.0002, 1: 0.0002, 2: 0.0002, 5: 0.9992, 8: 0.0002}, "[['-', '-', '-'], ['-', '-', 'O'], ['-', 'X', '-']]": {0: 0.00014285714285714287, 1: 0.00014285714285714287, 2: 0.00014285714285714287, 3: 0.00014285714285714287, 4: 0.00014285714285714287, 6: 0.00014285714285714287, 8: 0.9991428571428571}, "[['O', '-', 'X'], ['-', '-', 'O'], ['-', 'X', '-']]": {1: 0.2, 3: 0.2, 4: 0.2, 6: 0.2, 8: 0.2}, "[['O', '-', 'X'], ['-', 'O', 'O'], ['-', 'X', 'X']]": {1: 0.3333333333333333, 3: 0.3333333333333333, 6: 0.3333333333333333}, "[['O', '-', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']]": {1: 1.0}}
['O', '-', 'X']
['-', '-', 'O']
['-', 'X', '-']
ValuesByAction for state: [['O', '-', 'X'], ['-', '-', 'O'], ['-', 'X', '-']] --> {1: 0.0, 3: 0.0, 4: 0.0, 6: 0.0, 8: 0.0}
values --> [0.0, 0.0, 0.0, 0.0, 0.0]
sorted indices: [0 1 2 3 4]
UPDATE POLICY:  ASTAR ==> 8
POSSIBLE ACTIONS: {1, 3, 4, 6, 8}
UPDATED POLICY: {"[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]": {0: 0.00011111111111111112, 1: 0.00011111111111111112, 2: 0.00011111111111111112, 3: 0.00011111111111111112, 4: 0.00011111111111111112, 5: 0.00011111111111111112, 6: 0.00011111111111111112, 7: 0.9991111111111111, 8: 0.00011111111111111112}, "[['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']]": {0: 0.00014285714285714287, 1: 0.00014285714285714287, 2: 0.00014285714285714287, 3: 0.00014285714285714287, 5: 0.00014285714285714287, 7: 0.9991428571428571, 8: 0.00014285714285714287}, "[['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']]": {0: 0.0002, 1: 0.0002, 2: 0.0002, 5: 0.9992, 8: 0.0002}, "[['-', '-', '-'], ['-', '-', 'O'], ['-', 'X', '-']]": {0: 0.00014285714285714287, 1: 0.00014285714285714287, 2: 0.00014285714285714287, 3: 0.00014285714285714287, 4: 0.00014285714285714287, 6: 0.00014285714285714287, 8: 0.9991428571428571}, "[['O', '-', 'X'], ['-', '-', 'O'], ['-', 'X', '-']]": {1: 0.0002, 3: 0.0002, 4: 0.0002, 6: 0.0002, 8: 0.9992}, "[['O', '-', 'X'], ['-', 'O', 'O'], ['-', 'X', 'X']]": {1: 0.3333333333333333, 3: 0.3333333333333333, 6: 0.3333333333333333}, "[['O', '-', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']]": {1: 1.0}}
['O', '-', 'X']
['-', 'O', 'O']
['-', 'X', 'X']
ValuesByAction for state: [['O', '-', 'X'], ['-', 'O', 'O'], ['-', 'X', 'X']] --> {1: 0.0, 3: 0.0, 6: 0.0}
values --> [0.0, 0.0, 0.0]
sorted indices: [0 1 2]
UPDATE POLICY:  ASTAR ==> 6
POSSIBLE ACTIONS: {1, 3, 6}
UPDATED POLICY: {"[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]": {0: 0.00011111111111111112, 1: 0.00011111111111111112, 2: 0.00011111111111111112, 3: 0.00011111111111111112, 4: 0.00011111111111111112, 5: 0.00011111111111111112, 6: 0.00011111111111111112, 7: 0.9991111111111111, 8: 0.00011111111111111112}, "[['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']]": {0: 0.00014285714285714287, 1: 0.00014285714285714287, 2: 0.00014285714285714287, 3: 0.00014285714285714287, 5: 0.00014285714285714287, 7: 0.9991428571428571, 8: 0.00014285714285714287}, "[['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']]": {0: 0.0002, 1: 0.0002, 2: 0.0002, 5: 0.9992, 8: 0.0002}, "[['-', '-', '-'], ['-', '-', 'O'], ['-', 'X', '-']]": {0: 0.00014285714285714287, 1: 0.00014285714285714287, 2: 0.00014285714285714287, 3: 0.00014285714285714287, 4: 0.00014285714285714287, 6: 0.00014285714285714287, 8: 0.9991428571428571}, "[['O', '-', 'X'], ['-', '-', 'O'], ['-', 'X', '-']]": {1: 0.0002, 3: 0.0002, 4: 0.0002, 6: 0.0002, 8: 0.9992}, "[['O', '-', 'X'], ['-', 'O', 'O'], ['-', 'X', 'X']]": {1: 0.0003333333333333333, 3: 0.0003333333333333333, 6: 0.9993333333333333}, "[['O', '-', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']]": {1: 1.0}}
['O', '-', 'X']
['X', 'O', 'O']
['O', 'X', 'X']
ValuesByAction for state: [['O', '-', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']] --> {1: 0.0}
values --> [0.0]
sorted indices: [0]
UPDATE POLICY:  ASTAR ==> 1
POSSIBLE ACTIONS: {1}
UPDATED POLICY: {"[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]": {0: 0.00011111111111111112, 1: 0.00011111111111111112, 2: 0.00011111111111111112, 3: 0.00011111111111111112, 4: 0.00011111111111111112, 5: 0.00011111111111111112, 6: 0.00011111111111111112, 7: 0.9991111111111111, 8: 0.00011111111111111112}, "[['-', '-', '-'], ['-', 'X', '-'], ['O', '-', '-']]": {0: 0.00014285714285714287, 1: 0.00014285714285714287, 2: 0.00014285714285714287, 3: 0.00014285714285714287, 5: 0.00014285714285714287, 7: 0.9991428571428571, 8: 0.00014285714285714287}, "[['-', '-', '-'], ['O', 'X', '-'], ['O', 'X', '-']]": {0: 0.0002, 1: 0.0002, 2: 0.0002, 5: 0.9992, 8: 0.0002}, "[['-', '-', '-'], ['-', '-', 'O'], ['-', 'X', '-']]": {0: 0.00014285714285714287, 1: 0.00014285714285714287, 2: 0.00014285714285714287, 3: 0.00014285714285714287, 4: 0.00014285714285714287, 6: 0.00014285714285714287, 8: 0.9991428571428571}, "[['O', '-', 'X'], ['-', '-', 'O'], ['-', 'X', '-']]": {1: 0.0002, 3: 0.0002, 4: 0.0002, 6: 0.0002, 8: 0.9992}, "[['O', '-', 'X'], ['-', 'O', 'O'], ['-', 'X', 'X']]": {1: 0.0003333333333333333, 3: 0.0003333333333333333, 6: 0.9993333333333333}, "[['O', '-', 'X'], ['X', 'O', 'O'], ['O', 'X', 'X']]": {1: 1.0}}
